{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f3d9a2",
   "metadata": {},
   "source": [
    "## Fine tuning VGG16, VGG19 and ResNetRS101 for Human Activity Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f98fd2",
   "metadata": {},
   "source": [
    "### Group: 11 - Darshan Avaiya | Yukta Patel | Vaibhav Sheth | Taranjot Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fbfbf",
   "metadata": {},
   "source": [
    "### Professor - Ran Feldesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2a77e",
   "metadata": {},
   "source": [
    "Date - August 6, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027391b2",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a264e3",
   "metadata": {},
   "source": [
    "The aim of this project is to fine tune three CNN models for human activity classification. The project utilizes popular pre-trained models such as VGG16, VGG19, and ResNet101 to achieve high prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d01957",
   "metadata": {},
   "source": [
    "The project involves preprocessing input images to match the input size required by the models. The chosen model is loaded and fine-tuned for the specific image classification task. The models' architectures are adjusted by freezing certain layers to prevent overfitting. Additionally, techniques such as dropout and regularization are employed to enhance the models' generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac7ce5",
   "metadata": {},
   "source": [
    "The Human Activity Recognition (HAR) dataset used for training and evaluation consists of labeled images belonging to different classes. The models are trained on a subset of the dataset and validated on another subset. Dataset contations 15 different classes presenting various human activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bed066",
   "metadata": {},
   "source": [
    "The project's report highlights the technical aspects of implementing deep learning models, including model selection, architecture, preprocessing, and training. It emphasizes the results achieved through experimentation and provides insights into the challenges and solutions encountered during the project. By focusing on accurate image classification using deep learning, this project contributes to the broader field of computer vision and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551a651",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd9821",
   "metadata": {},
   "source": [
    "In the field of computer vision and artificial intelligence, image classification stands as a fundamental task with wide range of applications. Accurate and automated recognition of objects and activities within images is important for numerous domains, including healthcare, surveillance, and entertainment. The Image Classification Project seeks to address this task through the implementation of cutting-edge deep learning techniques, leveraging well-established pre-trained models such as VGG16, VGG19, and ResNetRS101."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa2826",
   "metadata": {},
   "source": [
    "The primary objective of this project is to develop robust and accurate models capable of classifying human activities into various predefined categories. The project's focus lies in understanding the training process of these deep learning models, their adaptation for the given dataset, and the effective implementation of image classification methodologies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f6170",
   "metadata": {},
   "source": [
    "The core problem addressed in this project is Human Activity Recognition (HAR) using image classification techniques. HAR involves the identification and categorization of different human activities based on images. The goal is to develop a deep learning model capable of accurately classifying images into predefined categories that correspond to various human activities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8427d",
   "metadata": {},
   "source": [
    "Human Activity Recognition (HAR) through image classification holds significant importance due to its wide-ranging applications and potential benefits in various domains such as healthcare, fitness and sports, survelliance and security, industrial safety and many more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b322e",
   "metadata": {},
   "source": [
    "## Related work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d2274",
   "metadata": {},
   "source": [
    "In the domain of Human Activity Recognition (HAR) and image classification, several prior works have laid the foundation for your project by exploring similar problems, methodologies, and challenges. Here are some examples of related work in this domain:\n",
    "\n",
    "1. \"Deep Convolutional Neural Networks for Human Activity Recognition Using Mobile Sensors\" (2015): This paper introduced the concept of using deep convolutional neural networks (CNNs) for HAR using sensor data from mobile devices. While not focused on images, it paved the way for applying deep learning to HAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33981a26",
   "metadata": {},
   "source": [
    "2. \"Human Activity Recognition: A Review\" (2019): This comprehensive review article summarizes the evolution of HAR techniques, including both traditional machine learning approaches and deep learning methods. It discusses datasets, features, and challenges in HAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb589b",
   "metadata": {},
   "source": [
    "3. \"Deep Learning-Based Human Activity Recognition: A Survey\" (2021): Another survey paper provides an up-to-date overview of deep learning techniques applied to HAR. It discusses various architectures, datasets, and challenges associated with training deep models for HAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf01fa4",
   "metadata": {},
   "source": [
    "4. Competitions and Challenges: Various data science competitions, such as Kaggle challenges, have provided platforms for researchers to showcase their HAR models. Studying winning solutions and approaches from such challenges can provide insights into effective strategies. Even dataset taken for this project is from a chellenge hosted by AI Planet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca26191",
   "metadata": {},
   "source": [
    "## Use Cases of Human Activity Classification/Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bce476",
   "metadata": {},
   "source": [
    "1. **Healthcare and Wellness Monitoring:** HAR can be utilized to monitor human activities for healthcare purposes. It can aid in tracking patient movements, identifying anomalies in daily activities, and ensuring the well-being of individuals, especially the elderly or those with medical conditions.\n",
    "\n",
    "2. **Fitness and Sports Analysis:** HAR can be employed in fitness tracking and sports analysis. It enables the assessment of exercise routines, tracking of performance metrics, and providing personalized feedback to individuals aiming for physical fitness and athletic excellence.\n",
    "\n",
    "3. **Surveillance and Security:** HAR is crucial for security and surveillance applications. It can assist in identifying suspicious or abnormal activities in public spaces, ensuring public safety, and enhancing security measures.\n",
    "\n",
    "4. **Industrial Safety:** In industrial settings, HAR can be used to monitor worker activities and ensure compliance with safety protocols. It helps prevent accidents and provides insights into optimizing workflow efficiency.\n",
    "\n",
    "5. **Behavioral Analysis:** HAR aids in understanding human behavior and patterns. This is valuable in fields like psychology, sociology, and market research, enabling researchers to gather insights into daily routines and habits.\n",
    "\n",
    "6. **Automated Processes:** In automation and robotics, HAR contributes to creating smarter systems that can react appropriately to human actions. This is essential in scenarios like industrial automation, robot-human collaboration, and autonomous vehicles.\n",
    "\n",
    "7. **Smart Environments:** HAR contributes to the development of smart environments that respond intelligently to human presence and actions. This includes energy-efficient lighting, climate control, and home automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5b142",
   "metadata": {},
   "source": [
    "## Introduction to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c360dad5",
   "metadata": {},
   "source": [
    "The dataset used in this project is collected from the Data Sprint 76 - Human Activity Recognition challenge provided by Aiplanet. It comprises a diverse collection of images capturing human activities, each associated with specific labels that denote the corresponding activity. The dataset is structured to enable supervised learning, where images are the input features, and the activity labels serve as the ground truth for model training and assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ff146",
   "metadata": {},
   "source": [
    "Dataset includes a train and test folders including images and a csv file containing training images names and associated lables with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843458b",
   "metadata": {},
   "source": [
    "While performing data preprocessing steps, image size was reduced to 120x120 according to computational capacity. Images were converted into numpy array and labels were encoded in one-hot vector to train models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3e56c",
   "metadata": {},
   "source": [
    "Training data was splitted into two parts. 70% dataset for training purpose and 30% of data was preparedfor model validation purpose. Stratified train validation splitting was used to maintain the balance in classes of human activities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0da5d",
   "metadata": {},
   "source": [
    "**Data Augmentation:** To increase diversity and control overfitting, following data augmentation was applied on training set.\n",
    "data augmentation can be understood as follows:\n",
    "\n",
    "1. **Rotation:** The images are rotated by a certain angle (up to the specified rotation range). This helps the model become invariant to different orientations of the activities.\n",
    "\n",
    "2. **Shift:** Images are shifted horizontally and vertically within a specified range. This simulates variations in camera angles and positioning.\n",
    "\n",
    "3. **Shear:** Shearing involves shifting one part of an image in a specific direction while keeping the other part fixed. This can simulate different viewpoints.\n",
    "\n",
    "4. **Zoom:** Images can be zoomed in or out, mimicking scenarios where the camera captures activities from different distances.\n",
    "\n",
    "5. **Horizontal Flip:** Images are flipped horizontally. This is particularly useful when the orientation of the activity doesn't impact its classification.\n",
    "\n",
    "6. **Fill Mode:** This specifies how to fill in the gaps created by transformations like rotation or shifting. The 'nearest' mode fills gaps with the nearest pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa16440",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae285dcb",
   "metadata": {},
   "source": [
    "Three models were trained using Tensorflow framework of Python:\n",
    "\n",
    "1. VGG16\n",
    "2. VGG19\n",
    "2. ResNetRS101\n",
    "\n",
    "Weights of top layers were freezed and few layers were added as top layers on architecture of above models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf962cf2",
   "metadata": {},
   "source": [
    "Top layers of all models were as per below:\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15, activation=\"softmax\"))\n",
    "\n",
    "20 epochs were used to train models with \"adam\" optimizer, \"categorical_crossentropy\" as a loss function and \"accuracy\" as evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b9f50",
   "metadata": {},
   "source": [
    "### 1. VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feaa9cc",
   "metadata": {},
   "source": [
    "VGG16, as its name suggests, is a 16-layer deep neural network. VGG16 is thus a relatively extensive network with a total of 138 million parameters—it’s huge even by today’s standards. However, the simplicity of the VGG16 architecture is its main attraction. For our dataset, number of classes is 15 and accordingly VGG16 contains 14.7 million parameters. New architecture with backbone of VGG16 contains 135 thousands trainable parameters.\n",
    " \n",
    "The VGGNet architecture incorporates the most important convolution neural network features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe134a9",
   "metadata": {},
   "source": [
    "![VGG16 Architecture](../images/VGG16.png \"VGG16 Architecture\")\n",
    "Figure 1: Architecture of VGG16 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66b046",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e000475",
   "metadata": {},
   "source": [
    "### 2. VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd3efe",
   "metadata": {},
   "source": [
    "VGG19 is a variant of VGG model which in short consists of 19 layers (16 convolution layers, 3 Fully connected layer, 5 MaxPool layers and 1 SoftMax layer). For the dataset with 15 classes, it has around 20 million parameters and overall architecture has arond 135 thousand trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aece42ea",
   "metadata": {},
   "source": [
    "![VGG19 Architecture](../images/VGG19.png \"VGG19 Architecture\")\n",
    "Figure 2: Architecture of VGG19 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07fc6b",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa52ca",
   "metadata": {},
   "source": [
    "### 3. ResNetRS101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28815a1f",
   "metadata": {},
   "source": [
    "ResNet-101 is a convolutional neural network that is 101 layers deep. for HAR dataset, it has 61 Million Parameters. Overall our model architecture consists aounr 528 thousand trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c9b27",
   "metadata": {},
   "source": [
    "![ResNet101 Architecture](../images/ResNet101.png \"ResNet101 Architecture\")\n",
    "Figure 3: Architecture of ResNet101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fecd89",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a48198",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8a81b",
   "metadata": {},
   "source": [
    "To explore appropriate training methods and processes on above methods, techniques such as Dropouts, Image augmentation and L2 regularizations were used step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afdf013",
   "metadata": {},
   "source": [
    "### Model 1 : VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867420e0",
   "metadata": {},
   "source": [
    "Training and evaluations results of fine tuning the VGG16 model is as per below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20134375",
   "metadata": {},
   "source": [
    "![Model 1 results](../images/model_1.png \"Model 1 results\")\n",
    "Figure 4: Training and evaluation of model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71869f",
   "metadata": {},
   "source": [
    "We can notice that training accuracy is very high at around 67% and evaluation accuracy is at 47%. Which shows model is overfitting on training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5006f7d",
   "metadata": {},
   "source": [
    "### Model 2 : VGG16 with L2 regularization and data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32748ae7",
   "metadata": {},
   "source": [
    "![Model 2 results](../images/model_2.png \"Model 2 results\")\n",
    "Figure 5: Training and evaluation of model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23702e8b",
   "metadata": {},
   "source": [
    "We used L2 regularization with l2 = 0.01. Here we can notice that model's accuracy on training data and evaluation data is around 45%. Although it is not desired accuracy but we significantly reduced overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a6e12",
   "metadata": {},
   "source": [
    "### Model 3: VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222bab4a",
   "metadata": {},
   "source": [
    "![Model 3 results](../images/model_3.png \"Model 3 results\")\n",
    "Figure 6: Training and evaluation of model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf5854",
   "metadata": {},
   "source": [
    "Here using VGG19 model, we got acccuracy on training data around 69% and on validation data, it's 48%. Again VGG19 model is also overfitting on training dataset. So, let's try data augmentation and regularization on this model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d72b6",
   "metadata": {},
   "source": [
    "### Model 4 : VGG19 with regularization and data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aaa450",
   "metadata": {},
   "source": [
    "![Model 4 results](../images/model_4.png \"Model 4 results\")\n",
    "Figure 7: Training and evaluation of model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f23d64",
   "metadata": {},
   "source": [
    "Accuracy chart shows that model 4's accuracy on training data and validation data is nearly same. which shows we overcome the problem of model overfitting on training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98050a4",
   "metadata": {},
   "source": [
    "### Model 5 : ResNetRS101 with data augmentation and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0244e",
   "metadata": {},
   "source": [
    "As we saw in VGG16 and VGG19, data augmentation and L2 regularization with l2=0.01 significantly reduced overfitting problem. So, we decided to train ResNetRS101 model with same regularization and data augmentaion techniques. Here are results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f471f7b",
   "metadata": {},
   "source": [
    "![Model 5 results](../images/model_5.png \"Model 5 results\")\n",
    "Figure 8: Training and evaluation of model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1496d",
   "metadata": {},
   "source": [
    "Accuracy chart for this model shows that model is performing poor on training data but well on validation data. Which means model is underfitting on training data set. So, we might have to reduce the regularization penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f406d0",
   "metadata": {},
   "source": [
    "### Model 6 : ResNetRS101 with data augmentation and regularization with l2=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a88b8",
   "metadata": {},
   "source": [
    "We reduced regularization to l2 = 0.001 and got these results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82208286",
   "metadata": {},
   "source": [
    "![Model 6 results](../images/model_6.png \"Model 6 results\")\n",
    "Figure 9: Training and evaluation of model 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187d803",
   "metadata": {},
   "source": [
    "Training and evaluatiion accuracy of model 6 shows that again model is suffering from overfitting problem. So, again regularization penalty needs to be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05268550",
   "metadata": {},
   "source": [
    "### Model 7 : ResNetRS101 with data augmentation and regularization with l2=0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36bf2e",
   "metadata": {},
   "source": [
    "With increased regularization term, here are results of model 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f0ddb",
   "metadata": {},
   "source": [
    "![Model 7 results](../images/model_7.png \"Model 7 results\")\n",
    "Figure 10: Training and evaluation of model 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e33645",
   "metadata": {},
   "source": [
    "These charts shows that, by increasing regularization term we model 7 successfully overcome the overfitting problem. It is noticable that after 20 epochs, accuracy of model on training data and evaluation data shows signs of increase. So, higher number of epochs may results in higher accuracy for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c165db",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b04e9",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eaa884",
   "metadata": {},
   "source": [
    "Out of all three models, ResNetRs101 (model 7) gave highest accuracy of 65%, due to its deep architecture and higher number of parameters. Using robust and scalable human activity classification algorithms can lead to improved safety, better healthcare monitoring, enhanced user experiences, and increased efficiency in various applications. It demonstrates the practical impact of deep learning and computer vision techniques in solving real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e6b6e",
   "metadata": {},
   "source": [
    "| Model | Architecture | Data Augmentation | Regularization | Train Accuracy | Validation Accuracy |\n",
    "| ----- | ------------ | ----------------- | -------------- | -------------- | ------------------- |\n",
    "|Model 1 | VGG16        | No   | -  | 67.5% | 47.4% |\n",
    "|Model 2 | VGG16        | Yes  | l2=0.01  | 44.5% | 46.9% |\n",
    "|Model 3 | VGG19        | No   | -  | 69.2% | 48.2% |\n",
    "|Model 4 | VGG19        | Yes  | l2 = 0.01  | 45.0% | 47.0% |\n",
    "|Model 5 | ResNetRS101  | Yes  | l2 = 0.01  | 55.2% | 61.1% |\n",
    "|Model 6 | ResNetRS101  | Yes  | l2 = 0.001  | 83.0% | 64.9% |\n",
    "|Model 7 | ResNetRS101  | Yes  | l2 = 0.005  | 64.3% | 65.7% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges Faced:**\n",
    "\n",
    "**1. Data Variability:** The dataset contains images capturing various lighting conditions, camera angles, and human poses. This variability can impact the model's ability to generalize across different scenarios.\n",
    "\n",
    "**2. Data Preprocessing:** Preprocessing the images to ensure consistency in terms of size, aspect ratio, and color channels is crucial. The choice of preprocessing techniques can influence the model's performance.\n",
    "\n",
    "**3. Model Selection:** Selecting the appropriate pre-trained models (VGG16, VGG19, ResNet101) involves considerations such as model complexity, computational resources, and expected accuracy. Choosing the model that best suits the problem is vital.\n",
    "\n",
    "**4. Overfitting:** Preventing overfitting is a significant concern. Fine-tuning the selected models and implementing techniques like dropout and regularization are essential to ensure the model generalizes well to unseen data.\n",
    "\n",
    "**5. Validation and Metrics:** Selecting appropriate evaluation metrics to assess the model's performance is crucial. Balancing accuracy, precision, recall, and F1-score based on the project's goals is a challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b611be",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b0a79",
   "metadata": {},
   "source": [
    "VGG16 and VGG19 models are deployed using Stremlit and available to use. However, best performing model can not be deployed due to its large size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff2bd6",
   "metadata": {},
   "source": [
    "Deployed model - https://human-activity-classification.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab45ba",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995006f3",
   "metadata": {},
   "source": [
    "1. Documentaion of Tensorflow 2 API available at https://www.tensorflow.org/api_docs\n",
    "2. Understanding VGG16: Concepts, Architecture, and Performance available at https://datagen.tech/guides/computer-vision/vgg16/#\n",
    "3. VGG19 documentation available at https://www.mathworks.com/help/deeplearning/ref/vgg19.html\n",
    "4. https://www.kaggle.com/datasets/pytorch/resnet101\n",
    "5. Dataset - https://aiplanet.com/challenges/data-sprint-76-human-activity-recognition/233/overview/about\n",
    "6. Streamlit available at https://streamlit.io/\n",
    "7. GitHub Repo of Project - https://github.com/idarshan07/human-activity-classification "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
